{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tomer/dynamic-rechunking-RAG/myenv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import polars as pl\n",
    "dataset = load_dataset(\"microsoft/ms_marco\", \"v1.1\")\n",
    "\n",
    "data_dict = dataset[\"train\"].to_dict()  # Convert to a dictionary of columns\n",
    "            \n",
    "            # Convert the dictionary to a Polars DataFrame\n",
    "df = pl.DataFrame(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from tqdm import tqdm\n",
    "# Initialize the model\n",
    "model = SentenceTransformer('intfloat/e5-base-v2')\n",
    "\n",
    "# Initialize Pinecone\n",
    "pinecone_index_name=\"dynamic\"\n",
    "pinecone_api_key=\"pcsk_5SLo1E_SPpKBieD8wQRuRf9G7xfYrVWDLWUAkfRC3X5xjMpDhy7j8CH3SEen8kJiNbMjav\"\n",
    "pc=Pinecone(api_key=pinecone_api_key)\n",
    "\n",
    "# Create Pinecone index if it doesn't exist\n",
    "print(\"Creating a Pinecone index...\")\n",
    "existing_indexes = [index_info[\"name\"] for index_info in pc.list_indexes()]\n",
    "if pinecone_index_name not in existing_indexes:\n",
    "    pc.create_index(\n",
    "        name=pinecone_index_name,\n",
    "        dimension=768,\n",
    "        metric=\"cosine\",\n",
    "        spec=ServerlessSpec(\n",
    "            cloud=\"aws\",\n",
    "            region=\"us-east-1\"\n",
    "        )\n",
    "    )\n",
    "pinecone_index=pc.Index(pinecone_index_name)\n",
    "\n",
    "index=pc.Index(pinecone_index_name)\n",
    "\n",
    "batch_size=100\n",
    "\n",
    "to_upsert = []\n",
    "for i in tqdm(range(0, len(df), batch_size)):\n",
    "# Extract the batch of rows\n",
    "    batch = df.slice(i, batch_size)\n",
    "\n",
    "    # Extract passage texts and prepare metadata\n",
    "    texts_to_embed = []\n",
    "    metadata_list = []\n",
    "    for row_idx, item in enumerate(batch.select(\"passages\").to_numpy().flatten()):\n",
    "        print(item)\n",
    "\n",
    "        if isinstance(item, list) and len(item) > 1:\n",
    "            # Extract passages from the second element of the list\n",
    "            passage_texts = item[1]\n",
    "            if isinstance(passage_texts, list):\n",
    "                for passage in passage_texts:\n",
    "                    texts_to_embed.append(f\"passage: {passage}\")\n",
    "                    metadata_list.append({\"row_number\": i + row_idx})  # Same row index for all passages\n",
    "            else:\n",
    "                print(f\"Skipping row {i + row_idx}: 'passage_texts' is not a list.\")\n",
    "        else:\n",
    "            print(f\"Skipping row {i + row_idx}: Invalid 'passages' structure.\")\n",
    "\n",
    "    # Generate embeddings for all passages in the batch\n",
    "    embeddings = model.encode(texts_to_embed, convert_to_numpy=True, batch_size=batch_size)\n",
    "\n",
    "    # Prepare list of (id, vector, metadata) dictionaries for upsert\n",
    "    for passage_idx, (embedding, metadata) in enumerate(zip(embeddings, metadata_list)):\n",
    "        # Unique ID includes row number and passage index\n",
    "        doc_id = f\"row-{metadata['row_number']}-passage-{passage_idx}\"\n",
    "        to_upsert.append({\n",
    "            \"id\": doc_id,\n",
    "            \"values\": embedding.tolist(),\n",
    "            \"metadata\": metadata\n",
    "        })\n",
    "\n",
    "    # Upsert in Pinecone in batches\n",
    "    if len(to_upsert) >= batch_size:\n",
    "        pinecone_index.upsert(vectors=to_upsert)\n",
    "        to_upsert = []  # Reset the upsert list\n",
    "\n",
    "# Handle any remaining vectors\n",
    "if to_upsert:\n",
    "    pinecone_index.upsert(vectors=to_upsert)\n",
    "\n",
    "print(\"Dense indexing completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
