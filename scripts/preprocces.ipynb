{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import polars as pl\n",
    "dataset = load_dataset(\"hugosousa/natural_questions_parsed\", split=\"train\", streaming=True)\n",
    "\n",
    "            \n",
    "            # Convert the dictionary to a Polars DataFrame\n",
    "# Retrieve and print the first row\n",
    "# Load the first 1000 rows\n",
    "first_1000_rows = []\n",
    "for i, row in enumerate(dataset):\n",
    "    if i >= 1000:\n",
    "        break\n",
    "    first_1000_rows.append(row)\n",
    "\n",
    "# Convert the list of dictionaries to a Polars DataFrame\n",
    "df = pl.DataFrame(first_1000_rows)\n",
    "\n",
    "# Save the DataFrame to a Parquet file\n",
    "df.write_parquet(\"/home/tomer/dynamic-rechunking-RAG/data/first_1000_rows.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating passages from the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['question', 'long_answer', 'short_answers', 'yes_no_answer', 'candidates', 'document', 'id']\n"
     ]
    }
   ],
   "source": [
    "import spacy.cli\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def divide_with_spacy(text, target_length=200):\n",
    "    \"\"\"\n",
    "    Divide text into passages using spaCy for sentence segmentation.\n",
    "\n",
    "    :param text: The input document as a string.\n",
    "    :param target_length: Approximate number of words per passage.\n",
    "    :return: A list of passages.\n",
    "    \"\"\"\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Extract sentences\n",
    "    sentences = [sent.text for sent in doc.sents]\n",
    "    \n",
    "    # Group sentences into passages\n",
    "    passages = []\n",
    "    current_passage = []\n",
    "    current_length = 0\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        words = sentence.split()\n",
    "        sentence_length = len(words)\n",
    "        \n",
    "        if current_length + sentence_length <= target_length:\n",
    "            current_passage.append(sentence)\n",
    "            current_length += sentence_length\n",
    "        else:\n",
    "            passages.append(\" \".join(current_passage))\n",
    "            current_passage = [sentence]\n",
    "            current_length = sentence_length\n",
    "\n",
    "    # Add the last passage\n",
    "    if current_passage:\n",
    "        passages.append(\" \".join(current_passage))\n",
    "\n",
    "    return passages\n",
    "print(df.columns)\n",
    "\n",
    "df = df.with_columns(\n",
    "    pl.col(\"document\").map_elements(lambda doc: divide_with_spacy(doc), return_dtype=pl.List(pl.Utf8)).alias(\"document_passages\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty dictionary\n",
    "import json\n",
    "df.write_parquet(\"/home/tomer/dynamic-rechunking-RAG/data/first_1000_rows_with_passeges.parquet\")\n",
    "passage_to_location = {}\n",
    "\n",
    "# Iterate over the DataFrame rows\n",
    "for row_idx, row in enumerate(df.iter_rows(named=True)):\n",
    "    passages = row[\"document_passages\"]  # Extract the list of passages\n",
    "    for passage_idx, passage in enumerate(passages):\n",
    "        # Map each passage to its document row and index\n",
    "        passage_to_location[f\"{row_idx}_{passage_idx}\"] = {\"passage\": passage, \"document_row\": row_idx, \"passage_index\": passage_idx}\n",
    "\n",
    "# Print the resulting dictionary\n",
    "json.dump(passage_to_location, open(\"/home/tomer/dynamic-rechunking-RAG/data/passage_to_location.json\", \"w\"), indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create embdedings and upserting to Vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating a Pinecone index...\n",
      "create embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/33467 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Deal - wikipedia New Deal Jump to: navigation , search This article is about the United States economic program. For other uses, see New Deal (disambiguation) . This article may be too long to read and navigate comfortably . The readable prose size is 95 kilobytes. Please consider splitting content into sub-articles, condensing it, or adding or removing subheadings . (October 2017)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 33467/33467 [08:55<00:00, 62.55it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upserting the embeddings to the Pinecone index...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 523/523 [19:28<00:00,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dense indexing completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "# Initialize the model\n",
    "model = SentenceTransformer('intfloat/e5-base-v2')\n",
    "\n",
    "# Initialize Pinecone\n",
    "pinecone_index_name=\"dynamic\"\n",
    "pinecone_api_key=\"\"\n",
    "pc=Pinecone(api_key=pinecone_api_key)\n",
    "\n",
    "# Create Pinecone index if it doesn't exist\n",
    "print(\"Creating a Pinecone index...\")\n",
    "existing_indexes = [index_info[\"name\"] for index_info in pc.list_indexes()]\n",
    "if pinecone_index_name not in existing_indexes:\n",
    "    pc.create_index(\n",
    "        name=pinecone_index_name,\n",
    "        dimension=768,\n",
    "        metric=\"cosine\",\n",
    "        spec=ServerlessSpec(\n",
    "            cloud=\"aws\",\n",
    "            region=\"us-east-1\"\n",
    "        )\n",
    "    )\n",
    "pinecone_index=pc.Index(pinecone_index_name)\n",
    "with open(\"/home/tomer/dynamic-rechunking-RAG/data/passage_to_location.json\", \"r\") as file:\n",
    "    data = json.load(file)\n",
    "index=pc.Index(pinecone_index_name)\n",
    "to_upsert=[]\n",
    "batch_size = 64\n",
    "print(\"create embeddings...\")\n",
    "for key, value in tqdm(data.items()):\n",
    "    passage = value.get(\"passage\", \"\")\n",
    "    if key==\"0_0\":\n",
    "        print(passage)\n",
    "    document_row = value.get(\"document_row\", None)\n",
    "    passage_index = value.get(\"passage_index\", None) \n",
    "    if passage:\n",
    "        embedding = model.encode(passage)\n",
    "    else:\n",
    "        embedding = None\n",
    "        continue        # Check if the \"passages\" structure is valid\n",
    "\n",
    "        # Prepare upsert data for the current row\n",
    "    to_upsert.append({\n",
    "        \"id\": str(key),\n",
    "        \"values\": embedding.tolist(),\n",
    "        \"metadata\": {\"row_number\":document_row, \"location\": passage_index, \"passage\":passage}       \n",
    "    })\n",
    "            \n",
    "        # Upsert data to Pinecone\n",
    "        \n",
    "       \n",
    "# Upsert in batches using tqdm for progress tracking\n",
    "print(\"Upserting the embeddings to the Pinecone index...\")\n",
    "for i in tqdm(range(0, len(to_upsert), batch_size)):\n",
    "    i_end = min(i + batch_size, len(to_upsert))\n",
    "    batch = to_upsert[i:i_end]\n",
    "    pinecone_index.upsert(vectors=batch)\n",
    "print(\"Dense indexing completed.\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
